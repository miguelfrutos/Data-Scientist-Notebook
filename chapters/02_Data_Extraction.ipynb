{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/Users/miguelfrutossoriano/Desktop/git/Notebook-Competition/assets/miguel_frutos.jpg\"  width=150, height=200 align=\"right\" form= 'circle' />\n",
    "\n",
    "# DATA SCIENCE NOTEBOOK COMPETITION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "\n",
    "We will need to use some modules to leverage the ML ecosystem. Check out in below cell, which libraries we will implement. if needed, install the libraries with the following command in your terminal.\n",
    "\n",
    "```pip install -r /path/to/requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Math & Stats packages\n",
    "import math\n",
    "from math import sqrt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import yeojohnson\n",
    "from scipy.stats import kruskal\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import add_constant\n",
    "\n",
    "# Random package\n",
    "import random\n",
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "# Sklearn: Prime ML library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn_pandas import  DataFrameMapper\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier, KernelDensity, DistanceMetric\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier,\\\n",
    "                                    IsolationForest, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder,\\\n",
    "                                    LabelEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split,\\\n",
    "                                    GridSearchCV, learning_curve, validation_curve, ShuffleSplit, \\\n",
    "                                    LeaveOneOut, KFold, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, matthews_corrcoef, \\\n",
    "                                    mean_squared_error, r2_score, mean_absolute_error, \\\n",
    "                                    roc_auc_score, roc_curve, accuracy_score, \\\n",
    "                                    confusion_matrix, multilabel_confusion_matrix, classification_report\n",
    "\n",
    "# Tensorflow & Keras for DL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import models, layers  \n",
    "from keras_visualizer import visualizer  \n",
    "\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns; sns.set()\n",
    "import pydotplus\n",
    "import graphviz\n",
    "import visualizer\n",
    "\n",
    "# Get a copy of data\n",
    "from copy import deepcopy\n",
    "from copy import copy\n",
    "\n",
    "# Ignore some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Missing values\n",
    "import missingno as msno\n",
    "\n",
    "# Special Dtypes\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "# Factor Analysis\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# For Markdown visualization\n",
    "from IPython.display import Image, display, Math, Latex\n",
    "\n",
    "#Over/Under Sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Extraction\n",
    "\n",
    "When we think about Data Engineering, ETL is the powerhouse and a core building block of most Data Warehouses. And while it is becoming a popular subject among all and is as simple as it sounds, an inside look in its processes is often something that most don’t mention about. If we peek at the first step of ETL, i.e. the Extraction of Data, it is a very important and crucial process. Until and unless you have the correct and structured data, you can neither clean it nor process it to be fetched by the Data Analysts or Data Scientists or, represented in the Software product.\n",
    "\n",
    "The process of Extraction is fully dependent on the Data Sources. It is important that these Data Sources are genuine and in a good shape that can add some value to the project’s main business ideology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction Techniques\n",
    "\n",
    "\n",
    "1. **CSV/Excel Data Sources**\n",
    "\n",
    "2. **Web-based Data sources**\n",
    "    1. Static Scraping\n",
    "    2. Dynamic Scraping\n",
    "    3. Web Scraping API’s and Softwares\n",
    "\n",
    "3. **PDF Data Sources**\n",
    "    1. Structured PDFs\n",
    "        1.1 Simple text PDFs\n",
    "        1.2 Tabular PDFs\n",
    "    2. Unstructured PDFs\n",
    "        2.1 Tabular PDFs\n",
    "    3. Images based PDFs\n",
    "    4. PDFs with tables\n",
    "\n",
    "\n",
    "4. **Unstructured Text-based**\n",
    "\n",
    "\n",
    "## Reference\n",
    "- [Data Extraction Techniques](https://medium.com/@sujayrittikar-10349/data-extraction-techniques-8cce04f94f23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **CSV/Excel Data Sources**\n",
    "\n",
    "The most common use case scenario is to have a dataset in .csv / .xls format. This is an advantage for us as the extraction process has been already done before and we just need to read the file and start working on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c68f4a",
   "metadata": {},
   "source": [
    "### CASE1: We have CSV files in our local CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bff6c64b",
   "metadata": {
    "id": "PI8K2zdyDhQh"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_main = pd.read_csv('./Notebook-Competition/datasets/package_test_main.csv')\n",
    "train_geo = pd.read_csv('./Notebook-Competition/datasets/package_train_geo.csv')\n",
    "train_weather = pd.read_csv('./Notebook-Competition/datasets/package_train_weather.csv')\n",
    "\n",
    "# test\n",
    "test_main = pd.read_csv('./Notebook-Competition/datasets/package_test_main.csv')\n",
    "test_geo = pd.read_csv('./Notebook-Competition/datasets/package_test_geo.csv')\n",
    "test_weather = pd.read_csv('./Notebook-Competition/datasets/package_test_weather.csv')\n",
    "\n",
    "# external\n",
    "holidays = pd.read_csv('./Notebook-Competition/datasets/package_holidays.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdefca",
   "metadata": {},
   "source": [
    "### CASE2: We have CSV files inside a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
