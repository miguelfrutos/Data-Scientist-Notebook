{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed2f012",
   "metadata": {
    "id": "YS9ghFH5CGVp"
   },
   "source": [
    "<img src=\"/Users/miguelfrutossoriano/Desktop/git/Notebook-Competition/assets/miguel_frutos.jpg\"  width=150, height=200 align=\"right\" form= 'circle' />\n",
    "\n",
    "# DATA SCIENCE NOTEBOOK COMPETITION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9a0ee",
   "metadata": {},
   "source": [
    "<a id=\"ch2\"></a>\n",
    "# A Data Science Framework\n",
    "\n",
    "1. **Frame the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve. Look at the Big Picture, define the problem, The first question to ask your boss is what exactly is the business objective; building a model is probably not the end goal, do we even need a ML model to find a solution? . How does the company expect to use and benefit from this model? This is important because it will determine how you frame the problem, what algorithms you will select, what performance measure you will use to evaluate your model, and how much effort you should spend tweaking it.\n",
    "\n",
    "2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are “drowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty data\" to \"clean data.\"\n",
    "\n",
    "3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn “wild” data into “manageable” data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n",
    "\n",
    "4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\n",
    "\n",
    "5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that’s used as actionable intelligence) at worst.\n",
    "\n",
    "6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\n",
    "\n",
    "7. **Optimize and Strategize:** This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your “currency exchange\" rate.\n",
    "\n",
    "# Resources\n",
    " - [LD Freeman - A Data Science Framework: To Achieve 99% Accuracy](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "\n",
    "We will need to use some modules to leverage the ML ecosystem. Check out in below cell, which libraries we will implement. if needed, install the libraries with the following command in your terminal.\n",
    "\n",
    "```pip install -r /path/to/requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Math & Stats packages\n",
    "import math\n",
    "from math import sqrt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import yeojohnson\n",
    "from scipy.stats import kruskal\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import add_constant\n",
    "\n",
    "# Random package\n",
    "import random\n",
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "# Sklearn: Prime ML library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn_pandas import  DataFrameMapper\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier, KernelDensity, DistanceMetric\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier,\\\n",
    "                                    IsolationForest, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder,\\\n",
    "                                    LabelEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split,\\\n",
    "                                    GridSearchCV, learning_curve, validation_curve, ShuffleSplit, \\\n",
    "                                    LeaveOneOut, KFold, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, matthews_corrcoef, \\\n",
    "                                    mean_squared_error, r2_score, mean_absolute_error, \\\n",
    "                                    roc_auc_score, roc_curve, accuracy_score, \\\n",
    "                                    confusion_matrix, multilabel_confusion_matrix, classification_report\n",
    "\n",
    "# Tensorflow & Keras for DL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import models, layers  \n",
    "from keras_visualizer import visualizer  \n",
    "\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns; sns.set()\n",
    "import pydotplus\n",
    "import graphviz\n",
    "import visualizer\n",
    "\n",
    "# Get a copy of data\n",
    "from copy import deepcopy\n",
    "from copy import copy\n",
    "\n",
    "# Ignore some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Missing values\n",
    "import missingno as msno  # # pip install missingno\n",
    "\n",
    "# Special Dtypes\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "# Factor Analysis\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# For Markdown visualization\n",
    "from IPython.display import Image, display, Math, Latex\n",
    "\n",
    "#Over/Under Sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
